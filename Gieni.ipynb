{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "h0EPX9TBwTq6",
   "metadata": {
    "id": "h0EPX9TBwTq6"
   },
   "source": [
    "# **OrderFox Hackathon: Domain-Specific Chatbot Design** üöÄ  \n",
    "\n",
    "## **üìå Overview**  \n",
    "In this hackathon, your goal is to design and implement a domain-specific chatbot using **Retrieval-Augmented Generation (RAG)**. You will be provided with a **dataset of HTML-crawled documents** in jason format, and your task is to build a system that effectively retrieves relevant information and generates accurate responses.  \n",
    "\n",
    "## **üîπ What You Need to Do**  \n",
    "1. **Set up your own environment** - A github Repository, or make a copy of this colab notebook, or a Kaggle notebook.\n",
    "2. **Load and Process Documents** ‚Äì Extract text from the json files provided to you.\n",
    "3. **Implement Document Retrieval**\n",
    "‚Äì Try different retrieval approaches, including:  \n",
    "   - Keyword-based search  \n",
    "   - Vector embeddings (e.g. Bag-of-Words, Word2Vec, embedding models on Hugging Face, embeddig models provided with OpenAI API)  + vector database\n",
    "   - Graph / Tree extraction + graph database\n",
    "   - Hybrid methods   \n",
    "4. **Perform Response Generation** ‚Äì Retrieve relevant documents and generate responses using a Language Model:\n",
    "  - Feel free to explore prompt engineer\n",
    "\n",
    "5. **Optimize and Evaluate Your System** ‚Äì Compare performance based on relevance, grounding, fluency, efficiency, and cost.  \n",
    "6. **Deliver a Well-Structured Solution** ‚Äì Organize your code as a modular project repository.  \n",
    " - Code repository or notebook\n",
    " - Make sure to include your knowledge database\n",
    "\n",
    "## **üì¶ What Is Provided?**  \n",
    "‚úÖ **Dataset**: JSON files containing extracted HTML content, available on a shared Google Drive and Kaggle.  \n",
    "‚úÖ **Baseline Notebook**: A starter kit with useful tools and guidance.  \n",
    "‚úÖ **Evaluation Metrics**: A structured evaluation framework to assess performance.  \n",
    "\n",
    "## **üñ•Ô∏è What Computing Resources Can You Use?**  \n",
    "You are welcome to choose your preferred platform to develop your solution. Either   \n",
    "- Locally (on your own machine) üíª  \n",
    "- or Using **cloud platforms** such as **Google Colab** or **Kaggle** (a free account is sufficient).  \n",
    "\n",
    "## **üõ†Ô∏è Tools You May Consider**  \n",
    "(*These are recommendations to help you get started. You are free to use alternative tools‚Äîjust document your choices clearly!*)  \n",
    "- **Database**: FAISS, ChromaDB, SQLite, Elasticsearch, Neo4j and etc.  \n",
    "- **Embedding Models**: Hugging Face Sentence-Transformers, OpenAI Embeddings  \n",
    "- **LLM for Generation**: OpenAI: gpt-4o-mini\n",
    "- **Others**: Langchain, GraphRAG, and etc.\n",
    "\n",
    "## **üìå Final Delivery**  \n",
    "Your final submission should include:  \n",
    "‚úÖ A well-documented **GitHub repository or notebook**  \n",
    "‚úÖ A clear **README** explaining your approach  \n",
    "‚úÖ A structured **retrieval and generation modules**  \n",
    "\n",
    "### **üî• Bonus Points For**  \n",
    "‚ú® Innovative retrieval techniques  \n",
    "‚ú® Well-organized, modular code  \n",
    "‚ú® Creative visualizations or user interfaces  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ipz5In6W_NIx",
   "metadata": {
    "id": "Ipz5In6W_NIx"
   },
   "source": [
    "# 1. Set up working environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dkuJ8NdPn8pc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkuJ8NdPn8pc",
    "outputId": "8e560343-0877-4b9d-bb84-b9d6582766d4"
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "\n",
    "# Database options\n",
    "!pip install chromadb # if you use chromadb as your vector database\n",
    "\n",
    "# Others\n",
    "!pip install langchain-community # if you use langchain for orchastration\n",
    "!pip install transformers #if you use huggingface for vector embedding\n",
    "\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80UAVJcCkCM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a80UAVJcCkCM",
    "outputId": "02284901-57af-47e1-b6f7-5346ca6f78a3"
   },
   "outputs": [],
   "source": [
    "# enable GPU if needed, GPU can speed up your vector embedding if you computing these vectors locally (not using API)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed41c7",
   "metadata": {
    "id": "20ed41c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQPuHKS4QQjn",
   "metadata": {
    "id": "EQPuHKS4QQjn"
   },
   "source": [
    "# 2. Knowledge Base Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C1ZmEq85_XvG",
   "metadata": {
    "id": "C1ZmEq85_XvG"
   },
   "source": [
    "## 2.1 Load documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GpLOcX8xoQ1",
   "metadata": {
    "id": "2GpLOcX8xoQ1"
   },
   "source": [
    "Once you are added access to this folder, it will appear at your google drive \"Shared drives\". Then you can mount your drive and as following, and access your data from \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\". Enjoy the ride! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kUvCnCVnRf5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUvCnCVnRf5c",
    "outputId": "4ffcfa7a-5269-48fa-d0d3-2e5a1b388200"
   },
   "outputs": [],
   "source": [
    "# Load the Drive and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jQt4yk-y_LqU",
   "metadata": {
    "id": "jQt4yk-y_LqU"
   },
   "source": [
    "Load json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AdSRf4xB_KWM",
   "metadata": {
    "id": "AdSRf4xB_KWM"
   },
   "outputs": [],
   "source": [
    "def load_document(json_file):\n",
    "    \"\"\"Loads the JSON file.\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "      try:\n",
    "          data = json.load(f)\n",
    "          return data\n",
    "      except json.JSONDecodeError:\n",
    "          print(f\"Error reading {json_file}, it may not be a valid JSON file.\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7rVVlmZ3KMTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rVVlmZ3KMTJ",
    "outputId": "cd8ece4f-1de8-42a2-8a13-5e2f3e41c56a"
   },
   "outputs": [],
   "source": [
    "folder_path = \"./hackathon_data/\"\n",
    "files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "len(files_in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9uC0u8QB9YX",
   "metadata": {
    "id": "r9uC0u8QB9YX"
   },
   "outputs": [],
   "source": [
    "def get_full_path(file):\n",
    "  return os.path.join(folder_path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTUgoion_dLj",
   "metadata": {
    "id": "LTUgoion_dLj"
   },
   "source": [
    "## 2.2 Pre-process documents.\n",
    "\n",
    "Feel free to explore and pre-process the data. You may want to clean or segment the documents as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yI1cBtnDyOx0",
   "metadata": {
    "id": "yI1cBtnDyOx0"
   },
   "outputs": [],
   "source": [
    "def page_segment(docs):\n",
    "    \"\"\"You may prefer to load each page separately.\"\"\"\n",
    "    i = 0\n",
    "    page_segment = []\n",
    "    for s in list(docs['text_by_page_url'].values()):\n",
    "      page_segment.append({\"docID\": docs['doc_id'], \"pageID\": 'page_' + str(i), \"text\": s})\n",
    "      i += 1\n",
    "    return page_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sjl7Xp1Pw6ps",
   "metadata": {
    "id": "Sjl7Xp1Pw6ps"
   },
   "outputs": [],
   "source": [
    "def segment_documents(docs, chunk_size=500):\n",
    "    \"\"\"Segments documents into chunks of a given token size. Replace this function with your segmentation approach or maybe use the original document without segmentation.\"\"\"\n",
    "    segmented = []\n",
    "    for doc_id, content in docs.items():\n",
    "        for i in range(0, len(content), chunk_size):\n",
    "            segment = content[i : i + chunk_size]\n",
    "            segmented.append({\"id\": doc_id, \"text\": segment})\n",
    "    return segmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x_3mbRONy7M7",
   "metadata": {
    "id": "x_3mbRONy7M7"
   },
   "outputs": [],
   "source": [
    "def document_clean(docs):\n",
    "  \"\"\"\n",
    "  You may want to clean the dataset, add the code here.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1MDaKaafPe4",
   "metadata": {
    "id": "j1MDaKaafPe4"
   },
   "outputs": [],
   "source": [
    "def document_should_skip(url):\n",
    "  extensions = [\".js\", \".css\", \".pdf\", \".csv\", \".doc\", \".docx\"]\n",
    "  for ext in extensions:\n",
    "    if ext in url:\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W4m4TC3QeJmJ",
   "metadata": {
    "id": "W4m4TC3QeJmJ"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/drive/MyDrive/DatathonDataset\n",
    "!mkdir -p /content/drive/MyDrive/DatathonDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrhr__MeK-hY",
   "metadata": {
    "id": "qrhr__MeK-hY"
   },
   "source": [
    "## 2.3 Document Indexing and Storage (Profiling)\n",
    "\n",
    "Feel free to choose different ways to indexing and storing the provided documents in a knowledge database.\n",
    "\n",
    "So that they can be retrieved in different ways according to your system design choices, such as search by keywords, vector representation, graph relation, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpICcaacAQSB",
   "metadata": {
    "id": "wpICcaacAQSB"
   },
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': device}\n",
    "encode_kwargs = {'normalize_embeddings': True, 'batch_size': 256}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zk4e8KL9AN4J",
   "metadata": {
    "id": "Zk4e8KL9AN4J"
   },
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"gieni\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./gieni_db_2\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B1x2nNijBwsW",
   "metadata": {
    "id": "B1x2nNijBwsW"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63jPU9YtFl",
   "metadata": {
    "id": "9d63jPU9YtFl"
   },
   "outputs": [],
   "source": [
    "def data_get_url(data):\n",
    "  if 'url' in data:\n",
    "    return data['url']\n",
    "  if 'website_url' in data:\n",
    "    return data['website_url']\n",
    "  raise Exception(\"URL not found\")\n",
    "\n",
    "def data_get_id(data):\n",
    "  if 'doc_id' in data:\n",
    "    return data['doc_id']\n",
    "  raise Exception(\"ID not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6TVsa8DXCMA3",
   "metadata": {
    "id": "6TVsa8DXCMA3"
   },
   "outputs": [],
   "source": [
    "def json_to_documents(file_name):\n",
    "  data = load_document(get_full_path(file_name))\n",
    "  docs = []\n",
    "  uuids = []\n",
    "  for i, (url, text) in enumerate(data['text_by_page_url'].items()):\n",
    "    if document_should_skip(url):\n",
    "      continue\n",
    "    try:\n",
    "      uuid = data_get_id(data) + \"_\" + str(i)\n",
    "      metadata = {\"company_url\": data_get_url(data), \"page_index\": i, \"uid\": uuid}\n",
    "      docs.append(Document(page_content=text, metadata=metadata, id=uuid))\n",
    "      uuids.append(uuid)\n",
    "    except Exception as e:\n",
    "      print(url, e)\n",
    "  return (docs, uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5boXWgGDmwP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "x5boXWgGDmwP",
    "outputId": "0ef48193-8aa0-435e-cf8a-0379c2bf7d9f"
   },
   "outputs": [],
   "source": [
    "# for i,file_name in enumerate(files_in_folder[:100]):\n",
    "#   docs, uuids = json_to_documents(file_name)\n",
    "#   vector_store.add_documents(documents=docs, ids=uuids)\n",
    "#   print(i)\n",
    "\n",
    "batch_files = 64\n",
    "total = 2048\n",
    "for i in tqdm(range(0, total, batch_files)):\n",
    "  docs = []\n",
    "  ids = []\n",
    "  for j in range(i, min(i+batch_files, len(files_in_folder))):\n",
    "    _docs, _ids = json_to_documents(files_in_folder[j])\n",
    "    docs = docs + _docs\n",
    "    ids = ids + _ids\n",
    "  print(f\"\\nAdding {len(docs)} documents\")\n",
    "  print(f\"Adding {sum([len(doc.page_content) for doc in docs])} characters\")\n",
    "  vector_store.add_documents(documents=docs, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ue_Qvt9DGizi",
   "metadata": {
    "id": "Ue_Qvt9DGizi"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "woyxfDBeN_mG",
   "metadata": {
    "id": "woyxfDBeN_mG"
   },
   "source": [
    "# 3. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSBIL9B-k1wE",
   "metadata": {
    "id": "hSBIL9B-k1wE"
   },
   "source": [
    "## 3.1 Load Knowledge Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LOy1k_-lOu0",
   "metadata": {
    "id": "7LOy1k_-lOu0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "X6jZ743wLayQ",
   "metadata": {
    "id": "X6jZ743wLayQ"
   },
   "source": [
    "## 3.2 Relevant Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj5pH-FATv6S",
   "metadata": {
    "id": "xj5pH-FATv6S"
   },
   "source": [
    "Feel free to check and improve your retrieval performance as it affect the generation results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a647547",
   "metadata": {
    "id": "6a647547"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, db_path, embedding_model):\n",
    "  \"\"\"\n",
    "  retrieve relevant documents from the knowledge database to the query.\n",
    "  \"\"\"\n",
    "  return relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kYJCpsgHLoc-",
   "metadata": {
    "id": "kYJCpsgHLoc-"
   },
   "source": [
    "## 3.3 Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TgE1qWd_UQA2",
   "metadata": {
    "id": "TgE1qWd_UQA2"
   },
   "source": [
    "Feel free to explore promp engineer to improve the quality of your generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhbpi5mcK3wy",
   "metadata": {
    "id": "mhbpi5mcK3wy"
   },
   "source": [
    "The retrieved documents are used as context to generate more relevant response. Gereral knowledge from the language model itself is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nacBKD6UJRNH",
   "metadata": {
    "id": "nacBKD6UJRNH"
   },
   "outputs": [],
   "source": [
    "def generate_answer(query, retrieved_texts, prompt_template):\n",
    "    \"\"\"Generates an answer using retrieved documents and GPT-4.\"\"\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6xIzmDTn3I8",
   "metadata": {
    "id": "-6xIzmDTn3I8"
   },
   "outputs": [],
   "source": [
    "query = \"What company is located in 29010 Commerce Center Dr., Valencia, 91355, California, US?\"\n",
    "retrieved_docs = retrieve_documents(query, db_path, embedding_model)\n",
    "response = generate_answer(query, retrieved_texts, prompt_template)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\", [doc.page_content for doc in retrieved_docs])\n",
    "print(\"Generated Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RK8CQELywSf1",
   "metadata": {
    "id": "RK8CQELywSf1"
   },
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSankY--nVHm",
   "metadata": {
    "id": "hSankY--nVHm"
   },
   "source": [
    "Try as many examples to evaluate your system and improve your performance!\n",
    "\n",
    "As the final sysrtem will be evaluated from various aspects. Try to check different metrics when you evaluate. One trick is to do a \"strict RAG\" where the response is generated based on the retrieved documents only, i.e. no general knowledge from the LLMs will be used. This may be a good way to check if your retrieval part is working as expected. Note, that in the final system general knowledge from the LLMs are welcome. \"Strict RAG\" is only used as a way for you to check your performance :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
