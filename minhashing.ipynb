{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Anze-/datathon2k25/blob/alberto/feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "1a7e08b9d04eedb7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **OrderFox Hackathon: Domain-Specific Chatbot Design** ðŸš€  \n",
    "\n",
    "## **ðŸ“Œ Overview**  \n",
    "In this hackathon, your goal is to design and implement a domain-specific chatbot using **Retrieval-Augmented Generation (RAG)**. You will be provided with a **dataset of HTML-crawled documents** in jason format, and your task is to build a system that effectively retrieves relevant information and generates accurate responses.  \n",
    "\n",
    "## **ðŸ”¹ What You Need to Do**  \n",
    "1. **Set up your own environment** - A github Repository, or make a copy of this colab notebook, or a Kaggle notebook.\n",
    "2. **Load and Process Documents** â€“ Extract text from the json files provided to you.\n",
    "3. **Implement Document Retrieval**\n",
    "â€“ Try different retrieval approaches, including:  \n",
    "   - Keyword-based search  \n",
    "   - Vector embeddings (e.g. Bag-of-Words, Word2Vec, embedding models on Hugging Face, embeddig models provided with OpenAI API)  + vector database\n",
    "   - Graph / Tree extraction + graph database\n",
    "   - Hybrid methods   \n",
    "4. **Perform Response Generation** â€“ Retrieve relevant documents and generate responses using a Language Model:\n",
    "  - Feel free to explore prompt engineer\n",
    "\n",
    "5. **Optimize and Evaluate Your System** â€“ Compare performance based on relevance, grounding, fluency, efficiency, and cost.  \n",
    "6. **Deliver a Well-Structured Solution** â€“ Organize your code as a modular project repository.  \n",
    " - Code repository or notebook\n",
    " - Make sure to include your knowledge database\n",
    "\n",
    "## **ðŸ“¦ What Is Provided?**  \n",
    "âœ… **Dataset**: JSON files containing extracted HTML content, available on a shared Google Drive and Kaggle.  \n",
    "âœ… **Baseline Notebook**: A starter kit with useful tools and guidance.  \n",
    "âœ… **Evaluation Metrics**: A structured evaluation framework to assess performance.  \n",
    "\n",
    "## **ðŸ–¥ï¸ What Computing Resources Can You Use?**  \n",
    "You are welcome to choose your preferred platform to develop your solution. Either   \n",
    "- Locally (on your own machine) ðŸ’»  \n",
    "- or Using **cloud platforms** such as **Google Colab** or **Kaggle** (a free account is sufficient).  \n",
    "\n",
    "## **ðŸ› ï¸ Tools You May Consider**  \n",
    "(*These are recommendations to help you get started. You are free to use alternative toolsâ€”just document your choices clearly!*)  \n",
    "- **Database**: FAISS, ChromaDB, SQLite, Elasticsearch, Neo4j and etc.  \n",
    "- **Embedding Models**: Hugging Face Sentence-Transformers, OpenAI Embeddings  \n",
    "- **LLM for Generation**: OpenAI: gpt-4o-mini\n",
    "- **Others**: Langchain, GraphRAG, and etc.\n",
    "\n",
    "## **ðŸ“Œ Final Delivery**  \n",
    "Your final submission should include:  \n",
    "âœ… A well-documented **GitHub repository or notebook**  \n",
    "âœ… A clear **README** explaining your approach  \n",
    "âœ… A structured **retrieval and generation modules**  \n",
    "\n",
    "### **ðŸ”¥ Bonus Points For**  \n",
    "âœ¨ Innovative retrieval techniques  \n",
    "âœ¨ Well-organized, modular code  \n",
    "âœ¨ Creative visualizations or user interfaces  \n"
   ],
   "metadata": {
    "id": "h0EPX9TBwTq6"
   },
   "id": "h0EPX9TBwTq6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Set up working environment"
   ],
   "metadata": {
    "id": "Ipz5In6W_NIx"
   },
   "id": "Ipz5In6W_NIx"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Knowledge Base Preparation"
   ],
   "metadata": {
    "id": "EQPuHKS4QQjn"
   },
   "id": "EQPuHKS4QQjn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Load documents"
   ],
   "metadata": {
    "id": "C1ZmEq85_XvG"
   },
   "id": "C1ZmEq85_XvG"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you are added access to this folder, it will appear at your google drive \"Shared drives\". Then you can mount your drive and as following, and access your data from \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\". Enjoy the ride! :)"
   ],
   "metadata": {
    "id": "2GpLOcX8xoQ1"
   },
   "id": "2GpLOcX8xoQ1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the Drive and mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUvCnCVnRf5c",
    "outputId": "f920db58-f4b9-4813-c0e5-8f8520c7a7cc",
    "ExecuteTime": {
     "end_time": "2025-04-05T18:54:25.150496Z",
     "start_time": "2025-04-05T18:54:25.148170Z"
    }
   },
   "id": "kUvCnCVnRf5c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load json file."
   ],
   "metadata": {
    "id": "jQt4yk-y_LqU"
   },
   "id": "jQt4yk-y_LqU"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "#folder_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\"# Google drive path of the dataset\n",
    "folder_path = \"data/hackathon_data\"\n",
    "files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "len(files_in_folder)"
   ],
   "metadata": {
    "id": "7rVVlmZ3KMTJ",
    "ExecuteTime": {
     "end_time": "2025-04-05T19:08:00.419408Z",
     "start_time": "2025-04-05T19:08:00.404488Z"
    }
   },
   "id": "7rVVlmZ3KMTJ",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13144"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def load_documents(json_file):\n",
    "    \"\"\"Loads the JSON file.\"\"\"\n",
    "    with open(json_file, 'r', encoding=\"utf-8\") as f:\n",
    "      try:\n",
    "          data = json.load(f)\n",
    "          return data\n",
    "      except json.JSONDecodeError:\n",
    "          print(f\"Error reading {json_file}, it may not be a valid JSON file.\")\n",
    "    return []\n",
    "\n"
   ],
   "metadata": {
    "id": "AdSRf4xB_KWM",
    "ExecuteTime": {
     "end_time": "2025-04-05T19:08:01.712712Z",
     "start_time": "2025-04-05T19:08:01.709576Z"
    }
   },
   "id": "AdSRf4xB_KWM",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "docs= []\n",
    "\n",
    "def document_should_skip(url):\n",
    "  extensions = [\".js\", \".css\", \".pdf\", \".csv\", \".doc\", \".docx\"]\n",
    "  for ext in extensions:\n",
    "    if ext in url:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "\n",
    "for filename in files_in_folder[:100]:\n",
    "    if filename.endswith('.json'):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        doc = load_documents(file_path)\n",
    "        for page_url, page in doc['text_by_page_url'].items():\n",
    "            if not document_should_skip(page_url):\n",
    "                docs.append(page)\n",
    "\n"
   ],
   "metadata": {
    "id": "PXf9s94Axg8U",
    "ExecuteTime": {
     "end_time": "2025-04-05T19:08:06.647529Z",
     "start_time": "2025-04-05T19:08:05.828459Z"
    }
   },
   "id": "PXf9s94Axg8U",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:22:42.727082Z",
     "start_time": "2025-04-05T19:22:42.656936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "set1 = set(['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'datasets'])\n",
    "set2 = set(['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "set3 = set(['minhash', 'is', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "\n",
    "m1 = MinHash(num_perm=128)\n",
    "m2 = MinHash(num_perm=128)\n",
    "m3 = MinHash(num_perm=128)\n",
    "for d in set1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in set2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in set3:\n",
    "    m3.update(d.encode('utf8'))\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "result = lsh.query(m1)\n",
    "print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)\n"
   ],
   "id": "68e12bb6f87a3494",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate neighbours with Jaccard similarity > 0.5 ['m2', 'm3']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:54:26.132072Z",
     "start_time": "2025-04-05T18:54:26.128060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg = 0\n",
    "for doc in docs:\n",
    "    avg +=len(doc)\n",
    "doc_avg = 500"
   ],
   "id": "eb16405f5db2da1b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:54:26.272024Z",
     "start_time": "2025-04-05T18:54:26.175695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    if len(doc) > doc_avg:\n",
    "        for i in range(0, len(doc), doc_avg):\n",
    "            chunked_docs.append(doc[i : i + doc_avg])\n",
    "    else:\n",
    "        chunked_docs.append(doc)"
   ],
   "id": "d7f68ededaa918d3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:54:26.284268Z",
     "start_time": "2025-04-05T18:54:26.280218Z"
    }
   },
   "cell_type": "code",
   "source": "len(chunked_docs)",
   "id": "e91a61819742b062",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110914"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:54:26.330589Z",
     "start_time": "2025-04-05T18:54:26.327622Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "12b440e4fb7e5b96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:56:12.075759Z",
     "start_time": "2025-04-05T18:54:26.380800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize GLiNER with the base model\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_small-v2.1\")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Labels for entity prediction\n",
    "# Most GLiNER models should work best when entity types are in lower case or title case\n",
    "labels = [\"Technology\", \"Service\", \"Material\", \"Product\",\"Industry\",\"Region\"]\n",
    "\n",
    "\n",
    "# Perform entity prediction\n",
    "# for chunked_doc in docs[:1024]:\n",
    "#     entities_to_chunked_docu = model.predict_entities(chunked_doc, [\"Technology\", \"Service\", \"Material\", \"Product\",\"Industry\",\"Region\"], threshold=0.6)\n",
    "\n",
    "model.run(chunked_docs, [\"Technology\", \"Service\", \"Material\", \"Product\",\"Industry\",\"Region\"], threshold=0.6, batch_size=32)"
   ],
   "id": "c418d3a068e22300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73f0ebf7cbdf412e9aa6f90195eec54e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 424 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 422 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 421 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 410 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 420 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 414 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 434 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 441 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 408 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 417 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 436 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 432 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 440 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 438 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 455 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 428 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 425 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 444 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 443 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 433 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 437 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 426 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 435 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 448 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 447 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 442 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 430 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 427 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 439 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 449 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 459 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/home/parf/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 454 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 5.92 GiB of which 876.69 MiB is free. Process 11392 has 4.76 MiB memory in use. Including non-PyTorch memory, this process has 4.09 GiB memory in use. Of the allocated memory 2.91 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     12\u001B[39m labels = [\u001B[33m\"\u001B[39m\u001B[33mTechnology\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mService\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mMaterial\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mProduct\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mIndustry\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mRegion\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# Perform entity prediction\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# for chunked_doc in docs[:1024]:\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m#     entities_to_chunked_docu = model.predict_entities(chunked_doc, [\"Technology\", \"Service\", \"Material\", \"Product\",\"Industry\",\"Region\"], threshold=0.6)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunked_docs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTechnology\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mService\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mMaterial\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mProduct\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mIndustry\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRegion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/model.py:353\u001B[39m, in \u001B[36mGLiNER.run\u001B[39m\u001B[34m(self, texts, labels, flat_ner, threshold, multi_label, batch_size)\u001B[39m\n\u001B[32m    350\u001B[39m             batch[key] = batch[key].to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# Perform predictions\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m model_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m    355\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model_output, torch.Tensor):\n\u001B[32m    356\u001B[39m     model_output = torch.from_numpy(model_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/modeling/base.py:238\u001B[39m, in \u001B[36mSpanModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, labels_embeddings, labels_input_ids, labels_attention_mask, words_embedding, mask, prompts_embedding, prompts_embedding_mask, words_mask, text_lengths, span_idx, span_mask, labels, **kwargs)\u001B[39m\n\u001B[32m    233\u001B[39m prompts_embedding, prompts_embedding_mask, words_embedding, mask = \u001B[38;5;28mself\u001B[39m.get_representations(input_ids, attention_mask, \n\u001B[32m    234\u001B[39m                                                                         labels_embeddings, labels_input_ids, labels_attention_mask, \n\u001B[32m    235\u001B[39m                                                                                                             text_lengths, words_mask)\n\u001B[32m    236\u001B[39m span_idx = span_idx*span_mask.unsqueeze(-\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m238\u001B[39m span_rep = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mspan_rep_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwords_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspan_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    240\u001B[39m prompts_embedding = \u001B[38;5;28mself\u001B[39m.prompt_rep_layer(prompts_embedding)\n\u001B[32m    242\u001B[39m scores = torch.einsum(\u001B[33m\"\u001B[39m\u001B[33mBLKD,BCD->BLKC\u001B[39m\u001B[33m\"\u001B[39m, span_rep, prompts_embedding)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/modeling/span_rep.py:356\u001B[39m, in \u001B[36mSpanRepLayer.forward\u001B[39m\u001B[34m(self, x, *args)\u001B[39m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, *args):\n\u001B[32m--> \u001B[39m\u001B[32m356\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mspan_rep_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/gliner/modeling/span_rep.py:286\u001B[39m, in \u001B[36mSpanMarkerV0.forward\u001B[39m\u001B[34m(self, h, span_idx)\u001B[39m\n\u001B[32m    282\u001B[39m end_span_rep = extract_elements(end_rep, span_idx[:, :, \u001B[32m1\u001B[39m])\n\u001B[32m    284\u001B[39m cat = torch.cat([start_span_rep, end_span_rep], dim=-\u001B[32m1\u001B[39m).relu()\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mout_project\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcat\u001B[49m\u001B[43m)\u001B[49m.view(B, L, \u001B[38;5;28mself\u001B[39m.max_width, D)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001B[39m, in \u001B[36mReLU.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001B[39m, in \u001B[36mrelu\u001B[39m\u001B[34m(input, inplace)\u001B[39m\n\u001B[32m   1702\u001B[39m     result = torch.relu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[32m   1703\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1704\u001B[39m     result = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1705\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 5.92 GiB of which 876.69 MiB is free. Process 11392 has 4.76 MiB memory in use. Including non-PyTorch memory, this process has 4.09 GiB memory in use. Of the allocated memory 2.91 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T18:47:19.339416Z",
     "start_time": "2025-04-05T18:46:24.931975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable = [\"ner\"])\n",
    "processed_docs = nlp.pipe(chunked_docs, n_process=1)\n",
    "\n",
    "doc_ents = []\n",
    "for doc in processed_docs:\n",
    "    doc_ents.append(doc.ents)"
   ],
   "id": "1892622d482f5584",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m processed_docs = nlp.pipe(chunked_docs, n_process=\u001B[32m1\u001B[39m)\n\u001B[32m      7\u001B[39m doc_ents = []\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprocessed_docs\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdoc_ents\u001B[49m\u001B[43m.\u001B[49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m.\u001B[49m\u001B[43ments\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/language.py:1621\u001B[39m, in \u001B[36mLanguage.pipe\u001B[39m\u001B[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001B[39m\n\u001B[32m   1619\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1620\u001B[39m         docs = pipe(docs)\n\u001B[32m-> \u001B[39m\u001B[32m1621\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/util.py:1703\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1693\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1694\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1695\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1700\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1701\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1702\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1703\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1704\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1705\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1706\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:245\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/util.py:1650\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1648\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1649\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1650\u001B[39m     batch = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mitertools\u001B[49m\u001B[43m.\u001B[49m\u001B[43mislice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitems\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1651\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1652\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/language.py:1618\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1615\u001B[39m     docs = \u001B[38;5;28mself\u001B[39m._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001B[32m   1616\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1617\u001B[39m     \u001B[38;5;66;03m# if n_process == 1, no processes are forked.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1618\u001B[39m     docs = (\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ensure_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts)\n\u001B[32m   1619\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1620\u001B[39m         docs = pipe(docs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/language.py:1131\u001B[39m, in \u001B[36mLanguage._ensure_doc\u001B[39m\u001B[34m(self, doc_like)\u001B[39m\n\u001B[32m   1129\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m doc_like\n\u001B[32m   1130\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1131\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmake_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_like\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m   1133\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m.vocab).from_bytes(doc_like)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/language.py:1123\u001B[39m, in \u001B[36mLanguage.make_doc\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1119\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) > \u001B[38;5;28mself\u001B[39m.max_length:\n\u001B[32m   1120\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1121\u001B[39m         Errors.E088.format(length=\u001B[38;5;28mlen\u001B[39m(text), max_length=\u001B[38;5;28mself\u001B[39m.max_length)\n\u001B[32m   1122\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1123\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:160\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:196\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:400\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/tokenizer.pyx:507\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._attach_tokens\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/vocab.pyx:205\u001B[39m, in \u001B[36mspacy.vocab.Vocab.get\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/vocab.pyx:234\u001B[39m, in \u001B[36mspacy.vocab.Vocab._new_lexeme\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/work/datathon2k25/.venv/lib/python3.12/site-packages/spacy/lang/lex_attrs.py:184\u001B[39m, in \u001B[36mget_lang\u001B[39m\u001B[34m(text, lang)\u001B[39m\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mis_stop\u001B[39m(string: \u001B[38;5;28mstr\u001B[39m, stops: Set[\u001B[38;5;28mstr\u001B[39m] = \u001B[38;5;28mset\u001B[39m()) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    181\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m string.lower() \u001B[38;5;129;01min\u001B[39;00m stops\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_lang\u001B[39m(text: \u001B[38;5;28mstr\u001B[39m, lang: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    185\u001B[39m     \u001B[38;5;66;03m# This function is partially applied so lang code can be passed in\u001B[39;00m\n\u001B[32m    186\u001B[39m     \u001B[38;5;66;03m# automatically while still allowing pickling\u001B[39;00m\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lang\n\u001B[32m    190\u001B[39m LEX_ATTRS = {\n\u001B[32m    191\u001B[39m     attrs.LOWER: lower,\n\u001B[32m    192\u001B[39m     attrs.NORM: lower,\n\u001B[32m   (...)\u001B[39m\u001B[32m    212\u001B[39m     attrs.LIKE_URL: like_url,\n\u001B[32m    213\u001B[39m }\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
